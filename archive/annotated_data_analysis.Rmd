---
title: "Annotated data analysis"
output: 
  html_document:
  toc: true
---

## Data cleaning
```{r}
setwd("D:/Dropbox/projects/2015lag/archive")

# Dependencies
library(stringr)
library(ggplot2)
library(knitr)
library(plyr)

# Read in data
dat <- read.csv(file = '20150514collected_data.csv')

# Assimilate journal naming
# See discrepancies
table(dat$data.journal)
# Most due to using PLoS and PLOS

# Solve this by making all journal names lowercase
dat$data.journal <- tolower(dat$data.journal)
table(dat$data.journal)

# plos medicin = typo
dat$data.journal[dat$data.journal == "plos medicin"] <- "plos medicine"

# remove the journals with "none"
dat <- dat[!dat$data.journal == "none", ]
table(dat$data.journal)

# Making the dates readable
dat$received <- as.Date(substring(dat$data.received_date,
                                  first = 0,
                                  last = 10))
dat$accepted <- as.Date(substring(dat$data.accepted_date,
                                  first = 0,
                                  last = 10))
dat$published <- as.Date(substring(dat$data.publication_date,
                                   first = 0,
                                   last = 10))

# Listwise deletion
# Nr of rows deleted
dim(dat)[1] - sum(!is.na(dat$received) &
                    !is.na(dat$accepted) &
                    !is.na(dat$published))

dat <- dat[!is.na(dat$received) &
             !is.na(dat$accepted) &
             !is.na(dat$published), ]

# Adding a year variable based on year published
dat$year <- as.numeric(substring(dat$data.publication_date,
                                 first = 0,
                                 last = 4))

# Adding the number of authors on the paper
# By counting the separators + 1 (for initial author)
# E.g., A; B counts 1 semi-colon, + 1 = 2 authors
dat$authors <- str_count(string = dat$data.author, ";")

# Adding a dummy for competing interests
# 0 = "no competing interests"
# 1 = !"no competing interest"
dat$coi <- !grepl(pattern = "no competing interests",
                  dat$data.competing_interest)
```

## Date checking
The data was cleaned in several steps, but the data were also inspected for some illogical dates, where the paper was published before it was accepted (i.e., `r sum(dat$published < dat$accepted)` cases) or accepted before received (i.e., `r sum(dat$accepted < dat$received)` cases). Below I give the DOIs for those papers and eliminate them from the dataset.

```{r}
# Checking for illogical dates
# Published before accepted
as.matrix(dat$data.id[dat$published < dat$accepted])
dat <- dat[!dat$published < dat$accepted, ]

# Accepted before received
as.matrix(dat$data.id[dat$accepted < dat$received])
dat <- dat[!dat$accepted < dat$received, ]
```
## Data prepping
### Computing days between received, accepted, and published
In order to actually conduct analyses on the publication lag, the dates for received-, accepted-, and published submission must be reformatted into difference data in days. The actual code running these computations is commented out to save time. The objects loaded in are saves from a previous run of the commented out code.

```{r}
# Calculating the days between received, accepted, and published
calc_days <- Vectorize(function(a, b) 
  length(seq(a, b, "days"))) 

# received_accepted <- calc_days(dat$received, dat$accepted)
# save(received_accepted, file = "received_accepted")
# 
# accepted_published <- calc_days(dat$accepted, dat$ published)
# save(accepted_published, file = "accepted_published")
# 
# received_published <- calc_days(dat$received, dat$published)
# save(received_published, file = "received_published")

# Load the objects created in the commented out code above
# Decreases runtime
load("received_accepted")
dat$received_accepted <- received_accepted
load("accepted_published")
dat$accepted_published <- accepted_published
load("received_published")
dat$received_published <- received_published
```

## Data analysis
Prior to data analysis I stress once again that the data *is* the population of PLOS research articles and that making inferences based on p-value hypothesis testing therefore makes no sense. I will therefore eliminate all hypothesis testing statistics and limit myself to point estimates.

### Descriptives
The median full publication cycle is `r median(dat$received_published)` days, with the majority of this being the review process (i.e., `r median(dat$received_accepted)` days) and not the production process (i.e., `r median(dat$accepted_published)` days). When we split this per journal, we see the following publication cycle.
```{r, echo = FALSE}
x <- ddply(.data = dat, .(data.journal),
           .fun = function(x) summary(x$received_published))
x[order(x$Median),]

```
which, when split up into the review process and production process looks as follows
```{r, echo = FALSE}
x <- ddply(.data = dat, .(data.journal),
           .fun = function(x) summary(x$received_accepted))
x[order(x$Median),]

x <- ddply(.data = dat, .(data.journal),
           .fun = function(x) summary(x$accepted_published))
x[order(x$Median),]
```
This indicates that the publication cycle is shortest for PLOS ONE, and longest for PLOS Medicine. This could be due to efficiency in handling more publications (i.e., ONE: `r table(dat$data.journal)[7]`; Med.: `r table(dat$data.journal)[5]`), but could also represent selectivity. PLOS ONE prouds itself of selecting papers only on scientific rigor and not on results, whereas PLOS medicine does include selectivity in its criteria for publication (e.g., originality of research; see their guidelines [here](http://journals.plos.org/plosmedicine/s/journal-information)).

```{r, fig.height = 10, echo = FALSE}
x <- ddply(dat, .(data.journal, year), function(x) summary(x$received_published))

par(mfrow = c(4, 2))
for(journal in unique(x$data.journal)){
  sel <- x[x$data.journal == journal, ]
  plot(x = min(sel$year):max(sel$year),
       y = sel$Median,
       xlab = "Year",
       ylab = "Median days",
       main = journal)
  }
```

These plots indicate that publication cycles have increased in length, except for Neglected Tropical Diseases, which shows a decreasing trend. Clinical trials was only published in 2006 and 2007, after which it was discontinued and rolled into PLOS ONE. Considering that the review process and production process are substantively different, it makes sense to investigate whether these trends differ across these parts of the publication cycle.

```{r, echo = FALSE, fig.height = 10}
x <- ddply(dat, .(data.journal, year), function(x) summary(x$received_accepted))

par(mfrow = c(4, 2))
for(journal in unique(x$data.journal)){
  sel <- x[x$data.journal == journal, ]
  plot(x = min(sel$year):max(sel$year),
       y = sel$Median,
       xlab = "Year",
       ylab = "Median days",
       main = journal)
  }
```

The trends for the review process seem highly similar to the overall. Considering that the review process takes up the largest part of the entire publication cycle (i.e., `r median(dat$received_accepted)` days of the full `r median(dat$  received_published)` days), it makes sense that the trends for the full publication cycle are mostly made up of the trends in the review process.

```{r, echo = FALSE, fig.height = 10}
x <- ddply(dat, .(data.journal, year), function(x) summary(x$accepted_published))

par(mfrow = c(4, 2))
for(journal in unique(x$data.journal)){
  sel <- x[x$data.journal == journal, ]
  plot(x = min(sel$year):max(sel$year),
       y = sel$Median,
       xlab = "Year",
       ylab = "Median days",
       main = journal)
  }
```

However, when we look at the production process, we see that the trends are relatively stable or decreasing. This indicates that some of the PLOS journals have increased the efficiency of the production process, whereas others have not. 

From these analyses I conclude two things in analyzing publication cycles:

1. The year of the publication should be taken into account
2. Not all PLOS journals are alike, so inspect whether the overall trend is valid for the journals separately.

### Correlational data analysis
The correlation between the time to go from received to accepted and the time to go from accepted to published is `r round(cor(received_accepted, accepted_published), 3)`. Note that competing interests is excluded from the correlation matrix because it is a dummy variable. Below the squared correlation matrix is given, for easier interpretation.

```{r}
d <- data.frame(receive_accept = as.numeric(received_accepted),
                accept_publish = as.numeric(accepted_published),
                receive_publish = as.numeric(received_published),
                authors = as.numeric(dat$authors),
                pages = as.numeric(dat$data.pagecount), 
                years = dat$year)
cor(d)^2
```

Here we see that squared correlations between the days from receive to accept and receive to published are high. This is logical because the majority of the publication cycle *is* the review process. The publication process, on the other hand, has only a medium correlation with the entire publication cycle.

Other squared correlations are all small. The largest uncontrolled effect is 1% explained variance. Controlling for other explanatory variables is likely to have only little effect, because of low squared correlations between the explanatory variables.

This indicates the regression analyses will most likely indicate that the effects of the predictor variables will be small and the publication process will prove highly random in its duration.

### Linearity
Because correlations rest on the assumption of linearity, let us check the linearity to ensure that we are not jumping the gun with the previous section (should actually do this before but I forgot...)

```{r, echo = FALSE, fig.height = 10}
pairs(x = d[,-6])
```

The plots of the predictor variables on the publication variables indicate curvilinear relations. This means that a linear correlation is insufficient to model the relationship.

### Regression models
Before running the regression models, let me recapitulate what the previous analyses indicated:

1. Publication year as covariate
2. Analyze review process and production process separately
3. Linear and curvilinear estimates for predictor variables.
4. Check whether overall results hold for journals separately

Additionally, I will mean center the predictor variables `authors` and `pages` so intercept estimates are meaningful. All regression models will be Poisson regression, considering that number of days in each part of publication cycle is a count variable.

```{r}
authors.centred <- dat$authors - mean(dat$authors, na.rm = TRUE)
authors.centred.sq <- authors.centred^2
pages.centred <- as.numeric(dat$data.pagecount) - mean(as.numeric(dat$data.pagecount), na.rm = TRUE)
pages.centred.sq <- pages.centred^2

x <- glm(received_accepted ~ 
           authors.centred +
           authors.centred.sq + 
           pages.centred + 
           pages.centred.sq +
           as.factor(year),
         data = dat,
         family = "poisson")

options(scipen = 5)
summary(x)
```

The intercept estimate is much to low for the average case in the data, indicating something might be going on in the modelling process. Some assumption might not hold for this data. Let's investigate.

### Investigating potential modeling issues
```{r, echo = FALSE, fig.height = 5}
plot(density(dat$received_accepted), main = "Distribution of days in review process")
lines(density(rpois(100000, x$coefficients[1])), lty = 3)
```

The above plot shows the distribution in the data (thin line) and the distribution estimated by the model (wide line). This clearly indicates that the variance is underestimated in the model. 

Note that, in Poisson models, the mean equals the variance (i.e., $\mu=\sigma^2=\lambda$; dispersion = 1). If we inspect the days in the review process, its mean is `r round(mean(dat$received_accepted), 3)` and its variance is `r round(var(dat$received_accepted), 3)`. In other words, the dispersion in the actual data is `r round(var(dat$received_accepted)/mean(dat$received_accepted), 3)`, which indicates severe overdispersion. If we input `family = "quasipoisson"` this solves the overdispersion, as it now is incorporated into the model. Doing this revealed to me that overdispersion only affects the standard errors of the estimates, so back to square one.

```{r}
plot(received_accepted)

x <- glm(received_accepted ~ 
        authors.centred +
        authors.centred.sq + 
        pages.centred + 
        pages.centred.sq +
        as.factor(year),
    data = dat)

int <- as.numeric(x$coefficients[1])
auth <- as.numeric(x$coefficients[2])
auth.sq <- as.numeric(x$coefficients[3])

curve(int +
        auth * x +
        auth.sq * x,
      add = TRUE,
      from = 1, to = 140000, col = "red")
x$coefficients
```